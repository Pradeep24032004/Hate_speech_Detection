{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMelePzUxcojqlxqXPWVqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradeep24032004/Hate_speech_Detection/blob/main/FineTuningBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGFTCE64nMmO"
      },
      "outputs": [],
      "source": [
        "# Regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Import torch.nn.functional as F\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data path\n",
        "data_path = \"/kaggle/input/davidsondataset2/labeled_data.csv\""
      ],
      "metadata": {
        "id": "hQspgPEqnN-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "data = pd.read_csv(data_path)\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
        "              'classk', 'tweet']\n",
        "df[\"classk\"] = df[\"classk\"].map({2:1, 1:0, 0:0})  # Convert labels to binary (hate vs not hate)\n",
        "tweets = df.tweet\n",
        "res = []"
      ],
      "metadata": {
        "id": "ROJPwPZYnYxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process tweets\n",
        "for i in range(len(tweets)):\n",
        "    splitt = tweets[i].split(\":\")\n",
        "    if len(splitt) == 1:\n",
        "        res.append(splitt[0])\n",
        "    else:\n",
        "        res.append(splitt[-1])\n",
        "\n",
        "tweets = res"
      ],
      "metadata": {
        "id": "LMV7bpZZnss-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Define hate and not_hate words\n",
        "hate_words = ['Moist', 'Cunt', 'Panties', 'Fuck', 'Hate', 'Nigger', 'Pussy', 'Ass',\n",
        "              'Motherfucker', 'Bitch', 'Damn']\n",
        "not_hate_words = ['Love', 'Peace', 'Kindness', 'Happiness', 'Respect', 'Friendship',\n",
        "                  'Appreciation', 'Hope', 'Encouragement', 'Support', 'Caring']"
      ],
      "metadata": {
        "id": "3apPDKxRnwAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tokenizer and BERT model\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "AQhWJm2-nwxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process hate words\n",
        "temp = []\n",
        "for word in hate_words:\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    outputs = torch.mean(outputs.last_hidden_state[0][1:], dim=0)\n",
        "    temp.append(outputs.tolist())\n",
        "\n",
        "hate_space = torch.tensor(temp).to(device).requires_grad_()\n",
        "\n",
        "# Process not_hate words\n",
        "temp = []\n",
        "for word in not_hate_words:\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    outputs = torch.mean(outputs.last_hidden_state[0][1:], dim=0)\n",
        "    temp.append(outputs.tolist())\n",
        "\n",
        "not_hate_space = torch.tensor(temp).to(device).requires_grad_()"
      ],
      "metadata": {
        "id": "3pA4PJ38nzPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights for hate_space and not_hate_space\n",
        "l = [[np.random.uniform(-1, 1)] for _ in range(11)]\n",
        "tensor_shape = (11, 1)\n",
        "hate_space_weights = torch.tensor(l).to(device).requires_grad_()\n",
        "not_hate_space_weights = torch.tensor(l).to(device).requires_grad_()"
      ],
      "metadata": {
        "id": "p0ss0tdAn24e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset class\n",
        "class Custom_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index: int) -> (torch.Tensor, int):\n",
        "        X = self.data[index]\n",
        "        y = self.labels[index]\n",
        "        return (X, y)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "iAY8EobIn5Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets, list(df.classk), test_size=0.2, shuffle=True)\n",
        "\n",
        "# Define train and test datasets and dataloaders\n",
        "train_data = Custom_Dataset(data=X_train, labels=y_train)\n",
        "test_data = Custom_Dataset(data=X_test, labels=y_test)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=3, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "o8VyE-3cn8CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model0(nn.Module):\n",
        "    def __init__(self, model, tokenizer):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.fc1 = nn.Linear(768, 256)  # Additional layer for fine-tuning\n",
        "        self.fc2 = nn.Linear(256, 2)     # Output layer\n",
        "\n",
        "    def forward(self, X):\n",
        "        inputs = self.tokenizer(X, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model(**inputs)\n",
        "        outputs = outputs.last_hidden_state.mean(1).transpose(0, 1)\n",
        "\n",
        "        # Additional layers for fine-tuning\n",
        "        outputs = torch.relu(self.fc1(outputs.squeeze()))  # Remove extra dimension\n",
        "        outputs = self.fc2(outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "9NjK0v3RoAmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model0 = Model0(model, tokenizer)\n",
        "model0.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(params=model0.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "euOp3FsaoDLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the inter and intra space loss functions\n",
        "def L_inter(spaces):\n",
        "    means = [torch.mean(spaces[i]) for i in range(spaces.shape[0])]\n",
        "    loss = torch.tensor(0.0, requires_grad=True)\n",
        "    for k in range(len(means)):\n",
        "        cur = torch.tensor(0.0, requires_grad=True)\n",
        "        for l in range(len(means)):\n",
        "            if l == k:\n",
        "                continue\n",
        "            cur = cur + (1 / (1 - ((means[k] * means[l]))))\n",
        "        loss = loss + cur\n",
        "    return loss\n",
        "\n",
        "def L_intra(spaces):\n",
        "    loss = torch.tensor(0.0, requires_grad=True)\n",
        "    for k in range(len(spaces)):\n",
        "        loss = loss + 1 / torch.var(spaces[k], dim=0)\n",
        "    return torch.sum(loss) / loss.shape[0]"
      ],
      "metadata": {
        "id": "Op5vf7BVoFjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model0.train()\n",
        "try:\n",
        "    for _ in range(3):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i % 2000 == 0:\n",
        "                print(i)\n",
        "\n",
        "            X, Y = batch\n",
        "            loss = 0\n",
        "\n",
        "            for X, y in zip(X, Y):\n",
        "                y_preds = model0(X)\n",
        "                loss += F.cross_entropy(y_preds.view(1, -1).to(device), torch.tensor([y]).to(device))\n",
        "\n",
        "            loss /= len(Y)\n",
        "            loss += 0.7 * L_inter(torch.cat((hate_space, not_hate_space))) + 0.5 * L_intra(torch.stack([hate_space, not_hate_space]))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted by the user.\")"
      ],
      "metadata": {
        "id": "gP6lcFQqoH_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = len(y_test)\n",
        "\n",
        "true = []\n",
        "preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i < 1000 == 0:\n",
        "            pass\n",
        "        X, y = batch\n",
        "        y_preds = model0(X)\n",
        "\n",
        "        # Move y tensor to the same device as y_preds\n",
        "        y = y.to(y_preds.device)\n",
        "\n",
        "        y_predict = torch.argmax(y_preds)\n",
        "\n",
        "        true.extend(y.cpu())  # Move true labels back to CPU\n",
        "        preds.append(y_predict.cpu())  # Move predictions back to CPU\n",
        "\n",
        "        cur = torch.eq(y_predict, y)\n",
        "\n",
        "        correct += torch.sum(cur)\n",
        "\n",
        "print(f\"Test Accuracy: {correct/total}\")"
      ],
      "metadata": {
        "id": "HJUZedSQoMty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}